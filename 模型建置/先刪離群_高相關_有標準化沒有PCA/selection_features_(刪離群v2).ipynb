{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6f2bd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Label\n",
      "0       boss\n",
      "1       boss\n",
      "2       boss\n",
      "3       boss\n",
      "4       boss\n",
      "...      ...\n",
      "67303  sport\n",
      "67304  sport\n",
      "67305  sport\n",
      "67306  sport\n",
      "67307  sport\n",
      "\n",
      "[67308 rows x 1 columns]\n",
      "       Eye_R_S_W  Eye_R_B_W  Eye_L_S_W  Eye_L_B_W  Eye_R_H1_W  Eye_R_H2_W  \\\n",
      "0       0.186630   0.223305   0.182876   0.220148    0.013553    0.032178   \n",
      "1       0.200132   0.244124   0.177286   0.207472    0.015152    0.038025   \n",
      "2       0.179414   0.210726   0.192387   0.231409    0.019380    0.045778   \n",
      "3       0.175631   0.208198   0.179902   0.219480    0.024991    0.057128   \n",
      "4       0.189342   0.230542   0.180274   0.213068    0.008799    0.020458   \n",
      "...          ...        ...        ...        ...         ...         ...   \n",
      "67303   0.189668   0.224763   0.198537   0.235773    0.010172    0.024601   \n",
      "67304   0.194105   0.231643   0.176857   0.212704    0.012763    0.030614   \n",
      "67305   0.191906   0.226189   0.189833   0.226757    0.013453    0.031954   \n",
      "67306   0.201010   0.241555   0.181272   0.215937    0.010469    0.024807   \n",
      "67307   0.188685   0.227093   0.182599   0.216482    0.008920    0.021947   \n",
      "\n",
      "       Eye_R_H3_W  Eye_R_H4_W  Eye_R_H5_W  Eye_R_H6_W  ...  face_R_widthS_L  \\\n",
      "0        0.047026    0.050299    0.041695    0.295160  ...         0.006584   \n",
      "1        0.056442    0.060386    0.050553    0.375086  ...         0.025190   \n",
      "2        0.066694    0.070605    0.057643    0.326996  ...         0.011141   \n",
      "3        0.082740    0.087278    0.069438    0.426920  ...         0.010561   \n",
      "4        0.030060    0.032070    0.027142    0.272649  ...         0.033373   \n",
      "...           ...         ...         ...         ...  ...              ...   \n",
      "67303    0.037045    0.040967    0.035552    0.293890  ...         0.004745   \n",
      "67304    0.045281    0.048855    0.041182    0.277546  ...         0.008964   \n",
      "67305    0.047510    0.051955    0.044398    0.280226  ...         0.011541   \n",
      "67306    0.036944    0.040215    0.034453    0.297930  ...         0.020875   \n",
      "67307    0.032850    0.035314    0.029840    0.279970  ...         0.014021   \n",
      "\n",
      "       face_L_width_L  face_L_widthS_L  Eye_to_eyebrow_L_L  \\\n",
      "0            0.507552         0.010848            0.069913   \n",
      "1            0.288393         0.025477            0.057711   \n",
      "2            0.487260         0.023674            0.050447   \n",
      "3            0.333769         0.017909            0.046187   \n",
      "4            0.543740         0.019484            0.056702   \n",
      "...               ...              ...                 ...   \n",
      "67303        0.535850         0.008899            0.075036   \n",
      "67304        0.513869         0.006003            0.060058   \n",
      "67305        0.600640         0.016062            0.051738   \n",
      "67306        0.473571         0.014753            0.058190   \n",
      "67307        0.581087         0.008531            0.073534   \n",
      "\n",
      "       eye_to_eyebrow_R_L  Nosehead_L  Forehead_w_L  nose_area  eye_R_area  \\\n",
      "0                0.072954    0.103485      0.935589   0.149271    0.009237   \n",
      "1                0.065030    0.086149      0.773583   0.145255    0.009582   \n",
      "2                0.052480    0.076988      0.767872   0.161162    0.010328   \n",
      "3                0.049003    0.051694      0.531466   0.190866    0.008599   \n",
      "4                0.065572    0.122033      1.172724   0.120627    0.007554   \n",
      "...                   ...         ...           ...        ...         ...   \n",
      "67303            0.080794    0.114940      0.938777   0.163385    0.007830   \n",
      "67304            0.057974    0.107795      0.991027   0.138025    0.009976   \n",
      "67305            0.071471    0.115522      1.017467   0.151713    0.010904   \n",
      "67306            0.062242    0.116505      1.069344   0.124224    0.008928   \n",
      "67307            0.072100    0.123893      1.124891   0.135496    0.008093   \n",
      "\n",
      "       eye_L_area  \n",
      "0        0.009100  \n",
      "1        0.008497  \n",
      "2        0.009207  \n",
      "3        0.009002  \n",
      "4        0.007722  \n",
      "...           ...  \n",
      "67303    0.008369  \n",
      "67304    0.007585  \n",
      "67305    0.010422  \n",
      "67306    0.006886  \n",
      "67307    0.008794  \n",
      "\n",
      "[67308 rows x 145 columns]\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv('./data cleaning/All_clean_IQR.csv')\n",
    "# df = df.drop(['face_width_w','face_length_L'],axis=1)\n",
    "df\n",
    "\n",
    "X = df.iloc[:, 4:] # Features\n",
    "y = df.iloc[:,3:4] # Label\n",
    "print(y)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6c1d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_feature_detect(data,threshold=0.75): # 特徵值之間相關係數很高(高度相關)\n",
    "    \"\"\" detect highly-correlated features of a Dataframe\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pd.Dataframe\n",
    "    threshold : threshold to identify the variable correlated\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pairs of correlated variables\n",
    "    \"\"\"\n",
    "    \n",
    "    corrmat = data.corr()\n",
    "    corrmat = corrmat.abs().unstack() # absolute value of corr coef\n",
    "    corrmat = corrmat.sort_values(ascending=False)\n",
    "    corrmat = corrmat[corrmat >= threshold]\n",
    "    corrmat = corrmat[corrmat < 1] # remove the digonal(對角線 corr=1)\n",
    "    corrmat = pd.DataFrame(corrmat).reset_index()\n",
    "    corrmat.columns = ['feature1', 'feature2', 'corr']\n",
    "   \n",
    "    grouped_feature_ls = []\n",
    "    correlated_groups = []\n",
    "    \n",
    "    for feature in corrmat.feature1.unique():\n",
    "        if feature not in grouped_feature_ls:\n",
    "    \n",
    "            # find all features correlated to a single feature\n",
    "            correlated_block = corrmat[corrmat.feature1 == feature]\n",
    "            grouped_feature_ls = grouped_feature_ls + list(\n",
    "                correlated_block.feature2.unique()) + [feature]\n",
    "    \n",
    "            # append the block of features to the list\n",
    "            correlated_groups.append(correlated_block)\n",
    "    return correlated_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef2a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 高相關 > 低變異\n",
    "# corr 0.75\n",
    "corr = corr_feature_detect(X,threshold=0.75)\n",
    "# print all the correlated feature groups!\n",
    "a= []\n",
    "for i in corr:\n",
    "    print(i,'\\n')\n",
    "    a.append(i['feature2'])\n",
    "#print(a)\n",
    "d=[]\n",
    "for ft in a:\n",
    "    for i in ft:\n",
    "        d.append(i)\n",
    "#print(d)\n",
    "deletlist = set(d)\n",
    "deletlist =list(deletlist)\n",
    "#print(deletlist)\n",
    "len(deletlist) #102\n",
    "X_c= X.drop(X[deletlist],axis=1)\n",
    "#X_c.shape #(66024, 34)\n",
    "\n",
    "\n",
    "# 低變異方差過濾 0.0005\n",
    "selector = VarianceThreshold(threshold=0.0005)\n",
    "X_cv = selector.fit_transform(X_c)  # 方差過濾\n",
    "\n",
    "#保留特徵名稱\n",
    "all_name = X_c.columns.values.tolist() \n",
    "select_name_index = selector.get_support(indices=True)  # 留下特徵的索引值，list格式\n",
    "select_name = []\n",
    "for i in select_name_index:\n",
    "    select_name.append(all_name[i])\n",
    "\n",
    "#X_cv.shape(66024, 8)\n",
    "print(select_name)\n",
    "\n",
    "# data normalization\n",
    "data_nor_X = preprocessing.StandardScaler().fit_transform(X_cv)\n",
    "data_nor_X\n",
    "\n",
    "#標準化的資料轉成pandas後csv存檔\n",
    "df_X = pd.DataFrame(data_nor_X)\n",
    "df_X.columns = select_name\n",
    "#pd.concat([df_X,y ],axis=1).to_csv('CorrVar_features.csv',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce0323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.高相關 > 卡方 (本次不適用 因為卡方要自己訂最後輸出特徵數)\n",
    "\n",
    "X_c.shape #已做完高相關選的 X : X_c\n",
    "\n",
    "#卡方 (k=20)\n",
    "selector =  SelectKBest(chi2,k=20)\n",
    "X_ck = selector.fit_transform(X_c,y)\n",
    "\n",
    "#保留特徵名稱\n",
    "all_name = X_c.columns.values.tolist() \n",
    "select_name_index2 = selector.get_support(indices=True)  # 留下特徵的索引值，list格式\n",
    "select_name2 = []\n",
    "for i in select_name_index2:\n",
    "    select_name2.append(all_name[i])\n",
    "select_name2 \n",
    "# X_ck.shape # (54825, 20)\n",
    "X_ck\n",
    "# data normalization\n",
    "data_nor_X = preprocessing.StandardScaler().fit_transform(X_ck)\n",
    "data_nor_X\n",
    "\n",
    "df_X2 = pd.DataFrame(data_nor_X,columns=select_name2)\n",
    "df_X2\n",
    "pd.concat([df_X2,y ],axis=1).to_csv('Corr_K_features.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db6d16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. 低變異> 高相關\n",
    "# 低變異方差過濾 0.0005\n",
    "selector = VarianceThreshold(threshold=0.0005)\n",
    "X_v = selector.fit_transform(X)  \n",
    "X_v\n",
    "#保留特徵名稱\n",
    "all_name = X.columns.values.tolist() \n",
    "select_name_index = selector.get_support(indices=True)  # 留下特徵的索引值，list格式\n",
    "select_name = []\n",
    "for i in select_name_index:\n",
    "    select_name.append(all_name[i])\n",
    "select_name\n",
    "# print(X_v.shape)  #(66024,38)\n",
    "X_v = pd.DataFrame(X_v,columns=select_name) #轉pd才能用相關係數判定\n",
    "# print(X_v)\n",
    "\n",
    "# corr 0.75\n",
    "corr = corr_feature_detect(X_v,threshold=0.75)\n",
    "# print all the correlated feature groups!\n",
    "a= []\n",
    "for i in corr:\n",
    "    print(i,'\\n')\n",
    "    a.append(i['feature2'])\n",
    "# print(a)\n",
    "d=[]\n",
    "for ft in a:\n",
    "     for i in ft:\n",
    "            d.append(i)\n",
    "#print(d)\n",
    "deletlist = set(d)\n",
    "deletlist =list(deletlist)\n",
    "# #print('deletlist:' , deletlist)\n",
    "# keep = X_vc.columns\n",
    "# keep = X_v.columns[keep] # keep col names\n",
    "X_vc= X_v.drop(X_v[deletlist],axis=1)\n",
    "X_vc.shape #(66024,9)\n",
    "\n",
    "# data normalization\n",
    "data_nor_X = preprocessing.StandardScaler().fit_transform(X_vc)\n",
    "data_nor_X\n",
    "\n",
    "#標準化的資料轉成pandas後csv存檔\n",
    "df_X = pd.DataFrame(data_nor_X,columns=X_vc.columns)\n",
    "#pd.concat([df_X,y ],axis=1).to_csv('VarCorr_features.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9734b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. 卡方> 高相關\n",
    "#卡方 k=50\n",
    "selector =  SelectKBest(chi2,k=50)\n",
    "X_k = selector.fit_transform(X,y)\n",
    "#保留特徵名稱\n",
    "all_name = X.columns.values.tolist() \n",
    "select_name_index = selector.get_support(indices=True)  # 留下特徵的索引值，list格式\n",
    "select_name4 = []\n",
    "for i in select_name_index:\n",
    "    select_name4.append(all_name[i])\n",
    "print(select_name4)\n",
    "\n",
    "#print(X_k.shape)#(66024, 50)\n",
    "\n",
    "X_k = pd.DataFrame(X_k,columns=select_name4) #轉pd才能用相關係數判定\n",
    "\n",
    "# corr 0.75\n",
    "corr = corr_feature_detect(X_k,threshold=0.75)\n",
    "# print all the correlated feature groups!\n",
    "a= []\n",
    "for i in corr:\n",
    "    print(i,'\\n')\n",
    "    a.append(i['feature2'])\n",
    "#print(a)\n",
    "d=[]\n",
    "for ft in a:\n",
    "    for i in ft:\n",
    "        d.append(i)\n",
    "#print(d)\n",
    "deletlist = set(d)\n",
    "deletlist =list(deletlist)\n",
    "#print('deletlist:' , deletlist)\n",
    "# keep = X_kc.columns\n",
    "X_kc= X_k.drop(X_k[deletlist],axis=1)\n",
    "X_kc.shape #(66024,12)\n",
    "\n",
    "# data normalization\n",
    "data_nor_X = preprocessing.StandardScaler().fit_transform(X_kc)\n",
    "data_nor_X\n",
    "\n",
    "#標準化的資料轉成pandas後csv存檔\n",
    "df_X = pd.DataFrame(data_nor_X, columns=X_kc.columns)\n",
    "\n",
    "#pd.concat([df_X,y ],axis=1).to_csv('K_Corr_features.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdef5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature1        feature2      corr\n",
      "0     Eye_dis_S_L     Eye_dis_B_L  0.998830\n",
      "200   Eye_dis_S_L      Nose_W_S_L  0.962291\n",
      "210   Eye_dis_S_L      Eye_L_H1_L  0.957629\n",
      "351   Eye_dis_S_L    Forehead_w_L  0.923808\n",
      "364   Eye_dis_S_L    face_width_L  0.921114\n",
      "420   Eye_dis_S_L      Nosehead_L  0.907592\n",
      "457   Eye_dis_S_L      Nose_W_M_L  0.902374\n",
      "587   Eye_dis_S_L   face_length_W  0.889079\n",
      "704   Eye_dis_S_L   Eyebrow_dis_L  0.880560\n",
      "752   Eye_dis_S_L       Eye_L_B_L  0.875394\n",
      "761   Eye_dis_S_L       Eye_R_B_L  0.873893\n",
      "792   Eye_dis_S_L       Eye_L_S_L  0.870992\n",
      "808   Eye_dis_S_L       Eye_R_S_L  0.868755\n",
      "923   Eye_dis_S_L     Lip_width_L  0.856989\n",
      "942   Eye_dis_S_L     Eyebrow_L_L  0.854634\n",
      "1062  Eye_dis_S_L     Eyebrow_R_L  0.838049\n",
      "1089  Eye_dis_S_L  Eyebrow_R_H2_W  0.834491\n",
      "1104  Eye_dis_S_L  Eyebrow_R_H1_W  0.832847\n",
      "1130  Eye_dis_S_L  Eyebrow_L_H4_W  0.829863\n",
      "1132  Eye_dis_S_L     face_down_W  0.829146\n",
      "1147  Eye_dis_S_L  Eyebrow_L_H5_W  0.827686\n",
      "1306  Eye_dis_S_L   face_middle_W  0.799892\n",
      "1400  Eye_dis_S_L       face_up_W  0.790069\n",
      "1460  Eye_dis_S_L  Eyebrow_R_H3_W  0.784600\n",
      "1495  Eye_dis_S_L  Eyebrow_L_H3_W  0.780126 \n",
      "\n",
      "       feature1   feature2      corr\n",
      "2     Lip_dh5_W  Lip_dh4_W  0.998103\n",
      "39    Lip_dh5_W  Lip_dh3_W  0.995333\n",
      "53    Lip_dh5_W  Lip_dh6_W  0.992399\n",
      "117   Lip_dh5_W  Lip_dh2_W  0.981999\n",
      "326   Lip_dh5_W  Lip_dh7_W  0.928087\n",
      "405   Lip_dh5_W  Lip_dh1_W  0.912237\n",
      "1546  Lip_dh5_W  Lip_dh4_L  0.772188\n",
      "1654  Lip_dh5_W  Lip_dh5_L  0.760117 \n",
      "\n",
      "        feature1    feature2      corr\n",
      "6     Eye_L_H5_W  Eye_L_H4_W  0.997359\n",
      "20    Eye_L_H5_W  Eye_L_H6_W  0.996981\n",
      "76    Eye_L_H5_W  Eye_L_H3_W  0.987594\n",
      "138   Eye_L_H5_W  Eye_L_H2_W  0.974328\n",
      "161   Eye_L_H5_W  Eye_L_H7_W  0.969860\n",
      "426   Eye_L_H5_W  Eye_R_H4_W  0.906194\n",
      "430   Eye_L_H5_W  Eye_R_H3_W  0.905489\n",
      "475   Eye_L_H5_W  Eye_R_H2_W  0.899929\n",
      "493   Eye_L_H5_W  Eye_R_H5_W  0.899104\n",
      "630   Eye_L_H5_W  Eye_R_H7_W  0.885812\n",
      "669   Eye_L_H5_W  Eye_L_H5_L  0.882758\n",
      "671   Eye_L_H5_W  Eye_L_H6_L  0.882643\n",
      "734   Eye_L_H5_W  Eye_L_H4_L  0.877347\n",
      "758   Eye_L_H5_W  Eye_R_H1_W  0.874645\n",
      "822   Eye_L_H5_W  Eye_L_H7_L  0.867661\n",
      "882   Eye_L_H5_W  Eye_L_H3_L  0.862504\n",
      "936   Eye_L_H5_W  eye_L_area  0.856101\n",
      "1030  Eye_L_H5_W  Eye_L_H2_L  0.842610\n",
      "1477  Eye_L_H5_W  Eye_R_H3_L  0.781916\n",
      "1491  Eye_L_H5_W  Eye_R_H4_L  0.780443\n",
      "1502  Eye_L_H5_W  Eye_R_H2_L  0.779260\n",
      "1564  Eye_L_H5_W  Eye_R_H5_L  0.769273\n",
      "1614  Eye_L_H5_W  Eye_R_H1_L  0.764838\n",
      "1635  Eye_L_H5_W  eye_R_area  0.761299 \n",
      "\n",
      "       feature1   feature2      corr\n",
      "32    Lip_dh3_L  Lip_dh4_L  0.995796\n",
      "59    Lip_dh3_L  Lip_dh5_L  0.991572\n",
      "106   Lip_dh3_L  Lip_dh2_L  0.982889\n",
      "183   Lip_dh3_L  Lip_dh6_L  0.965936\n",
      "981   Lip_dh3_L  Lip_dh1_L  0.850045\n",
      "1160  Lip_dh3_L  Lip_dh7_L  0.826142\n",
      "1563  Lip_dh3_L  Lip_dh2_W  0.769593\n",
      "1656  Lip_dh3_L  Lip_dh3_W  0.760024 \n",
      "\n",
      "        feature1    feature2      corr\n",
      "44    Eye_R_H7_L  Eye_R_H5_L  0.994562\n",
      "118   Eye_R_H7_L  Eye_R_H4_L  0.981941\n",
      "130   Eye_R_H7_L  eye_R_area  0.977404\n",
      "176   Eye_R_H7_L  Eye_R_H3_L  0.966826\n",
      "240   Eye_R_H7_L  Eye_R_H2_L  0.946150\n",
      "592   Eye_R_H7_L  Eye_R_H1_L  0.888870\n",
      "608   Eye_R_H7_L  Eye_L_H3_L  0.887810\n",
      "617   Eye_R_H7_L  Eye_L_H4_L  0.887510\n",
      "623   Eye_R_H7_L  Eye_R_H7_W  0.887041\n",
      "675   Eye_R_H7_L  Eye_L_H2_L  0.882512\n",
      "679   Eye_R_H7_L  Eye_L_H5_L  0.882308\n",
      "686   Eye_R_H7_L  eye_L_area  0.881509\n",
      "711   Eye_R_H7_L  Eye_R_H5_W  0.879564\n",
      "768   Eye_R_H7_L  Eye_L_H6_L  0.872940\n",
      "934   Eye_R_H7_L  Eye_R_H4_W  0.856139\n",
      "1066  Eye_R_H7_L  Eye_L_H7_L  0.836445\n",
      "1100  Eye_R_H7_L  Eye_R_H3_W  0.833577\n",
      "1315  Eye_R_H7_L  Eye_R_H2_W  0.799045\n",
      "1572  Eye_R_H7_L  Eye_L_H3_W  0.768746\n",
      "1586  Eye_R_H7_L  Eye_L_H2_W  0.766875\n",
      "1647  Eye_R_H7_L  Eye_L_H4_W  0.760591 \n",
      "\n",
      "         feature1     feature2      corr\n",
      "48    Eye_dis_B_W  Eye_dis_S_W  0.993841\n",
      "1331  Eye_dis_B_W   Nose_W_S_W  0.797571 \n",
      "\n",
      "         feature1     feature2      corr\n",
      "54    Lip_beads_W     Lip_h3_W  0.991931\n",
      "56    Lip_beads_W     Lip_h4_W  0.991909\n",
      "243   Lip_beads_W     Lip_h5_W  0.944859\n",
      "247   Lip_beads_W     Lip_h2_W  0.943542\n",
      "641   Lip_beads_W  Lip_beads_L  0.884826\n",
      "855   Lip_beads_W     Lip_h3_L  0.864859\n",
      "897   Lip_beads_W     Lip_h4_L  0.859864\n",
      "1468  Lip_beads_W     Lip_h6_W  0.783365\n",
      "1475  Lip_beads_W     Lip_h1_W  0.781979 \n",
      "\n",
      "           feature1        feature2      corr\n",
      "64   face_R_width_W  face_L_width_W  0.989178\n",
      "416  face_R_width_W     Eyebrow_R_W  0.908565\n",
      "543  face_R_width_W     Eyebrow_L_W  0.892676 \n",
      "\n",
      "        feature1        feature2      corr\n",
      "148   Eye_R_H6_W         N_REH_W  0.972035\n",
      "323   Eye_R_H6_W         N_RET_W  0.928819\n",
      "375   Eye_R_H6_W        Nose_H_W  0.919731\n",
      "714   Eye_R_H6_W   face_middle_W  0.879393\n",
      "1060  Eye_R_H6_W      Nose_W_B_L  0.838049\n",
      "1171  Eye_R_H6_W         N_LEH_W  0.823312\n",
      "1271  Eye_R_H6_W  Eyebrow_R_H3_W  0.805793\n",
      "1375  Eye_R_H6_W       face_up_W  0.792164\n",
      "1651  Eye_R_H6_W   face_length_W  0.760276\n",
      "1664  Eye_R_H6_W  Eyebrow_R_H1_W  0.759437\n",
      "1679  Eye_R_H6_W  Eyebrow_R_H2_W  0.758648 \n",
      "\n",
      "       feature1     feature2      corr\n",
      "162   Eye_L_B_W    Eye_L_S_W  0.968800\n",
      "1303  Eye_L_B_W  Eyebrow_L_W  0.800427 \n",
      "\n",
      "       feature1     feature2      corr\n",
      "184   Eye_R_B_W    Eye_R_S_W  0.964478\n",
      "1666  Eye_R_B_W  Eyebrow_R_W  0.759328 \n",
      "\n",
      "             feature1         feature2      corr\n",
      "212   face_R_widthS_W  face_R_widthS_L  0.956734\n",
      "817   face_R_widthS_W  face_L_widthS_W  0.868119\n",
      "1207  face_R_widthS_W  face_L_widthS_L  0.818040 \n",
      "\n",
      "     feature1        feature2      corr\n",
      "236   N_RET_L      Eye_R_H6_L  0.949254\n",
      "1071  N_RET_L         N_REH_L  0.836254\n",
      "1151  N_RET_L       Eye_R_S_L  0.827571\n",
      "1162  N_RET_L       Eye_R_B_L  0.826112\n",
      "1227  N_RET_L      Nose_W_B_W  0.814652\n",
      "1318  N_RET_L  face_R_width_L  0.798590\n",
      "1388  N_RET_L     Eyebrow_R_L  0.791574\n",
      "1419  N_RET_L     face_down_W  0.788531\n",
      "1421  N_RET_L      Nosehead_L  0.788304\n",
      "1577  N_RET_L  Eyebrow_R_H5_L  0.768233\n",
      "1701  N_RET_L      Eye_L_H1_L  0.754869\n",
      "1725  N_RET_L      Nose_W_S_L  0.750792 \n",
      "\n",
      "            feature1        feature2      corr\n",
      "259   face_L_width_L     Eyebrow_L_L  0.941232\n",
      "713   face_L_width_L       Eye_L_B_L  0.879503\n",
      "796   face_L_width_L      Nose_W_B_L  0.870053\n",
      "1042  face_L_width_L       Eye_L_S_L  0.840987\n",
      "1050  face_L_width_L  Eyebrow_L_H1_L  0.840186\n",
      "1220  face_L_width_L         N_LET_L  0.815189\n",
      "1604  face_L_width_L    face_width_L  0.765700\n",
      "1629  face_L_width_L    Forehead_w_L  0.761542 \n",
      "\n",
      "        feature1     feature2      corr\n",
      "269   Lip_peak_L  Lip_width_L  0.938192\n",
      "826   Lip_peak_L   Nose_W_M_L  0.867533\n",
      "1246  Lip_peak_L   Nosehead_L  0.810565 \n",
      "\n",
      "      feature1     feature2      corr\n",
      "276   Lip_h5_L     Lip_h4_L  0.936813\n",
      "447   Lip_h5_L  Lip_beads_L  0.903318\n",
      "552   Lip_h5_L     Lip_h6_L  0.891856\n",
      "696   Lip_h5_L     Lip_h3_L  0.880861\n",
      "1034  Lip_h5_L     Lip_h2_L  0.842054\n",
      "1435  Lip_h5_L     Lip_h5_W  0.787422 \n",
      "\n",
      "     feature1   feature2      corr\n",
      "356   N_LET_W    N_LEH_W  0.922887\n",
      "958   N_LET_W  nose_area  0.853836\n",
      "1454  N_LET_W   Nose_H_W  0.785200 \n",
      "\n",
      "            feature1        feature2      corr\n",
      "392   Eyebrow_L_H2_W  Eyebrow_L_H3_W  0.915530\n",
      "1229  Eyebrow_L_H2_W  Eyebrow_L_H4_W  0.813710\n",
      "1234  Eyebrow_L_H2_W  Eyebrow_L_H1_W  0.812305\n",
      "1583  Eyebrow_L_H2_W   face_middle_W  0.767182\n",
      "1662  Eyebrow_L_H2_W         N_LEH_W  0.759443\n",
      "1677  Eyebrow_L_H2_W        Nose_H_W  0.758769\n",
      "1708  Eyebrow_L_H2_W  Eyebrow_L_H5_W  0.753912 \n",
      "\n",
      "           feature1        feature2      corr\n",
      "397  Eyebrow_R_H4_L  Eyebrow_R_H5_L  0.915029\n",
      "988  Eyebrow_R_H4_L  Eyebrow_R_H3_L  0.848278 \n",
      "\n",
      "         feature1       feature2      corr\n",
      "402   face_down_L       Nose_H_L  0.913119\n",
      "748   face_down_L  face_middle_L  0.875543\n",
      "1717  face_down_L        N_REH_L  0.753208 \n",
      "\n",
      "       feature1     feature2      corr\n",
      "418  Lip_peak_W  Lip_width_W  0.907844 \n",
      "\n",
      "           feature1        feature2      corr\n",
      "451  Eyebrow_L_H2_L  Eyebrow_L_H1_L  0.902699\n",
      "977  Eyebrow_L_H2_L  Eyebrow_L_H3_L  0.851237 \n",
      "\n",
      "            feature1        feature2      corr\n",
      "471   Eyebrow_R_H4_W  Eyebrow_R_H3_W  0.900725\n",
      "1116  Eyebrow_R_H4_W  Eyebrow_R_H5_W  0.831799\n",
      "1443  Eyebrow_R_H4_W  Eyebrow_R_H2_W  0.786897 \n",
      "\n",
      "         feature1      feature2      corr\n",
      "506  Lip_height_L  Lip_height_W  0.897583 \n",
      "\n",
      "      feature1  feature2      corr\n",
      "530   Lip_h1_L  Lip_h2_L  0.894664\n",
      "1719  Lip_h1_L  Lip_h6_L  0.752463 \n",
      "\n",
      "     feature1  feature2      corr\n",
      "888   N_LEH_L  Nose_H_L  0.861967\n",
      "1073  N_LEH_L   N_LET_L  0.836193 \n",
      "\n",
      "            feature1        feature2     corr\n",
      "1020  Eyebrow_L_H5_L  Eyebrow_L_H4_L  0.84378 \n",
      "\n",
      "            feature1        feature2      corr\n",
      "1044  Eyebrow_R_H2_L  Eyebrow_R_H1_L  0.840706\n",
      "1457  Eyebrow_R_H2_L  Eyebrow_R_H3_L  0.784849 \n",
      "\n",
      "        feature1    feature2      corr\n",
      "1084  Nosehead_W  Nose_W_M_W  0.835337 \n",
      "\n",
      "                feature1            feature2      corr\n",
      "1200  Eye_to_eyebrow_L_W      Eyebrow_L_H5_W  0.818180\n",
      "1214  Eye_to_eyebrow_L_W  eye_to_eyebrow_R_W  0.815866\n",
      "1440  Eye_to_eyebrow_L_W      Eyebrow_L_H4_W  0.787214\n",
      "1570  Eye_to_eyebrow_L_W      Eyebrow_R_H1_W  0.768831 \n",
      "\n",
      "               feature1           feature2      corr\n",
      "1258  philtrum_length_L  philtrum_length_W  0.808491 \n",
      "\n",
      "           feature1    feature2      corr\n",
      "1661  Eyebrow_dis_W  Nose_W_S_W  0.759731 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5. 單純高相關過濾\n",
    "\n",
    "# corr 0.75\n",
    "corr = corr_feature_detect(X,threshold=0.75)\n",
    "# print all the correlated feature groups!\n",
    "a= []\n",
    "for i in corr:\n",
    "    print(i,'\\n')\n",
    "    a.append(i['feature2'])\n",
    "#print(a)\n",
    "d=[]\n",
    "for ft in a:\n",
    "    for i in ft:\n",
    "        d.append(i)\n",
    "#print(d)\n",
    "deletlist = set(d)\n",
    "deletlist =list(deletlist)\n",
    "#print('deletlist:' , deletlist)\n",
    "X_c= X.drop(X[deletlist],axis=1)\n",
    "X_c.shape #(66024,34)\n",
    "keep = X_c.columns\n",
    "# data normalization\n",
    "# data_nor_X = preprocessing.StandardScaler().fit_transform(X_c)\n",
    "# data_nor_X\n",
    "\n",
    "Stand = preprocessing.StandardScaler()\n",
    "data_nor_X=Stand.fit_transform(X_c)\n",
    "\n",
    "#標準化的資料轉成pandas後csv存檔\n",
    "df_X = pd.DataFrame(data_nor_X)\n",
    "df_X.columns = keep\n",
    "pd.concat([df_X,y],axis=1).to_csv('only_Corr_features.csv',index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a22b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open (\"Standard.pickle\",\"wb\") as f:\n",
    "    pickle.dump(Stand,f,protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2472006",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deletlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=X.columns.to_list()\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b223204",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in a:\n",
    "    if i in deletlist:\n",
    "        b.append(False)\n",
    "    else:\n",
    "        b.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#單純高相關 > PCA \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "pca = PCA(n_components=2) # ND -> 2D (Number of components to keep. if n_components is not set all components are kept:)\n",
    "result = pca.fit_transform(df_X) \n",
    "print('coefficient of each compoments:\\n{}'.format(pca.components_))\n",
    "labelencoder = LabelEncoder()\n",
    "y_data = labelencoder.fit_transform(df['Label'])\n",
    "plt.scatter(result[:,0], result[:,1], c=y_data, s=25, alpha=0.4, marker='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8280e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#單純高相關 > lDA \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "lda = LDA(n_components=2) # ND -> 2D (Number of components to keep. if n_components is not set all components are kept:)\n",
    "result = lda.fit_transform(df_X,y) \n",
    "result\n",
    "lda.explained_variance_ratio_\n",
    "\n",
    "#print('coefficient of each compoments:\\n{}'.format(lda.explained_variance_))\n",
    "# labelencoder = LabelEncoder()\n",
    "# y_data = labelencoder.fit_transform(df['Label'])\n",
    "# plt.scatter(result[:,0], result[:,1], c=y_data, s=25, alpha=0.4, marker='v')\n",
    "\n",
    "# plt.xlabel('LDA1')\n",
    "# plt.ylabel('LDA2')\n",
    "# label = np.unique(y)\n",
    "# plt.legend(label[:],loc='upper right')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6f773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x=result[:,0], \n",
    "                y=result[:,1], \n",
    "                hue=(df['Label']), \n",
    "                style=None, \n",
    "                size=None, \n",
    "                data=None, \n",
    "                palette=None, \n",
    "                sizes=None, \n",
    "                alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5240ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#T-sne\n",
    "from sklearn import datasets\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=0) # n_components降為幾維\n",
    "X_2d = tsne.fit_transform(df_X)\n",
    "\n",
    "print(X_2d.shape)\n",
    "\n",
    "# 畫圖需要將Label 轉為數字\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "y_data = labelencoder.fit_transform(df['Label'])\n",
    "plt.scatter(X_2d[:,0], X_2d[:,1], c=y_data, s=25, alpha=0.1, marker='o')\n",
    "plt.xlabel(\"tsne1\") \n",
    "plt.ylabel(\"tsne2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr+K > PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#抽樣\n",
    "\n",
    "df = pd.read_csv('./Corr_K_features.csv')\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "df = shuffle(df)\n",
    "df\n",
    "filter1=df['Label']=='boss'  #158\n",
    "filter2=df['Label']=='sport' #154\n",
    "filter3=df['Label']=='entertainer' #165\n",
    "filter4=df['Label']=='politician'#162\n",
    "filter5=df['Label']=='doctor'#157\n",
    "filter6=df['Label']=='ordinary_people'# 165\n",
    "\n",
    "sub1=df[filter1].sample(n=None, frac=0.05, replace=False, weights=None, random_state=8)\n",
    "sub2=df[filter2]\n",
    "sub3=df[filter3].sample(n=None, frac=0.03, replace=False, weights=None, random_state=8)\n",
    "sub4=df[filter4].sample(n=None, frac=0.15, replace=False, weights=None, random_state=8)\n",
    "sub5=df[filter5].sample(n=None, frac=0.25, replace=False, weights=None, random_state=8)\n",
    "sub6=df[filter6].sample(n=None, frac=0.003, replace=False, weights=None, random_state=8)\n",
    "df=pd.concat([sub1,sub2,sub3,sub4,sub5,sub6],axis=0)\n",
    "df_X2=df.iloc[:,:20]\n",
    "df_y=df['Label']\n",
    "\n",
    "\n",
    "pca2 = PCA(n_components=2) # ND -> 2D (Number of components to keep. if n_components is not set all components are kept:)\n",
    "result = pca2.fit_transform(df_X2) \n",
    "print('coefficient of each compoments:\\n{}'.format(pca.components_))\n",
    "labelencoder = LabelEncoder()\n",
    "y_data = labelencoder.fit_transform(df['Label'])\n",
    "plt.scatter(result[:,0], result[:,1], c=y_data, s=25, alpha=0.5, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21325f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca= PCA(n_components=None)\n",
    "# result = pca.fit_transform(df_X) \n",
    "# print('variance-ratio:',pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e65b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#單純高相關 > LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lda = LDA(n_components=2) # ND -> 2D (Number of components to keep. if n_components is not set all components are kept:)\n",
    "result = lda.fit_transform(df_X2,df_y) \n",
    "# print('coefficient of each compoments:\\n{}'.format(pca.components_))\n",
    "labelencoder = LabelEncoder()\n",
    "y_data = labelencoder.fit_transform(df['Label'])\n",
    "plt.scatter(result[:,0], result[:,1], c=y_data, s=25, alpha=0.6, marker='o')\n",
    "plt.xlabel('LDA1')\n",
    "plt.ylabel('LDA2')\n",
    "label = np.unique(df_y)\n",
    "label\n",
    "plt.legend(label,loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "label=np.unique(df_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(abs(pca1.components_))\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(df_X.columns)),df_X.columns,rotation=65,ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9f562",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(abs(pca2.components_))\n",
    "plt.yticks([0,1],['1st Comp','2nd Comp'],fontsize=10)\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(df_X2.columns)),df_X2.columns,rotation=65,ha='left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb475fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#單純高相關 > LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lda = LDA(n_components=2) # ND -> 2D (Number of components to keep. if n_components is not set all components are kept:)\n",
    "result = lda.fit_transform(df_X,y) \n",
    "lda.coef_\n",
    "# print('coefficient of each compoments:\\n{}'.format(lda.components_))\n",
    "# labelencoder = LabelEncoder()\n",
    "# y_data = labelencoder.fit_transform(df['Label'])\n",
    "# plt.scatter(result[:,0], result[:,1], c=y_data, s=25, alpha=0.4, marker='v')\n",
    "# plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56444df",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98a4261",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "#SVM\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_X, y_data, test_size=0.3)\n",
    "\n",
    "# we can change kernel to rbf, poly, linear\n",
    "model = SVC(kernel='linear', C=0.3) #c =Soft cost 懲罰參數 (p.52) kernal(p.58)\n",
    "model.fit(X_train, y_train)\n",
    "#前面PCA已先做過標準化 這裡忽略標準化step\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "num_correct_samples = accuracy_score(y_test, y_pred, normalize=False)\n",
    "con_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('number of correct sample: {}'.format(num_correct_samples))\n",
    "print('accuracy: {}'.format(accuracy))\n",
    "print('con_matrix: {}'.format(con_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47a87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(df_X, y_data)\n",
    "print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53d252",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
